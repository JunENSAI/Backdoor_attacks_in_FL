{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9304510f-4071-4c5d-af41-bb3283cd29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network (CNN) for the MNIST dataset.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Conv Layer 1: Captures low-level features (edges, lines).\n",
    "    2. Conv Layer 2: Captures high-level features (shapes, curves).\n",
    "    3. Dropout: Prevents overfitting (memorizing the data).\n",
    "    4. Fully Connected Layers: Makes the final classification decision (0-9).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor: Defines the layers (the 'tools') we will use.\n",
    "        We do not connect them here; we just initialize them.\n",
    "        \"\"\"\n",
    "        super(MNISTNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(9216, 128) # Dense layer: 9216 inputs -> 128 outputs\n",
    "        self.fc2 = nn.Linear(128, 10)   # Final layer: 128 inputs -> 10 outputs (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward Pass: Defines how data flows through the network.\n",
    "        This function is called automatically when you do model(data).\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input image batch of shape (Batch_Size, 1, 28, 28)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Log-probabilities for each class (0-9).\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Helper function to easily instantiate the model from other files.\"\"\"\n",
    "    return MNISTNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c15ca8-373a-4a62-8d9a-7cf816f95c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41e74fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "_DATA_CACHE = None\n",
    "\n",
    "def get_iid_partitions(dataset, num_clients, seed=1001):\n",
    "    \"\"\"\n",
    "    IID Helper: Randomly shuffles indices and splits them into equal chunks.\n",
    "    This replaces the complex Dirichlet logic from the Non-IID version.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    total_items = len(dataset)\n",
    "    indices = np.arange(total_items)\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    partitions = np.array_split(indices, num_clients)\n",
    "    \n",
    "    return [p.tolist() for p in partitions]\n",
    "\n",
    "def prepare_dataset(num_clients, seed=1001):\n",
    "    \"\"\"\n",
    "    Centralized data loader.\n",
    "    \"\"\"\n",
    "    global _DATA_CACHE\n",
    "    \n",
    "    if _DATA_CACHE is not None:\n",
    "        return _DATA_CACHE\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    user_groups = get_iid_partitions(train_dataset, num_clients, seed)\n",
    "\n",
    "    _DATA_CACHE = (train_dataset, test_dataset, user_groups)\n",
    "    return _DATA_CACHE\n",
    "\n",
    "def add_square_trigger(image, trigger_size=4, x_pos=24, y_pos=24, pixel_value=2.8):\n",
    "    \"\"\"\n",
    "    Applies a white square trigger to a single image tensor.\n",
    "    Pixel value 2.8 is approx max for normalized MNIST.\n",
    "    \"\"\"\n",
    "    poisoned_image = image.clone()\n",
    "    poisoned_image[:, x_pos:x_pos+trigger_size, y_pos:y_pos+trigger_size] = pixel_value\n",
    "    return poisoned_image\n",
    "\n",
    "def create_backdoor_test_set(test_dataset, target_label=0):\n",
    "    \"\"\"\n",
    "    Creates a dataset to measure Attack Success Rate (ASR).\n",
    "    Takes NON-target images, adds trigger, and labels them as target.\n",
    "    \"\"\"\n",
    "    poisoned_data = []\n",
    "    \n",
    "    for i in range(len(test_dataset)):\n",
    "        img, label = test_dataset[i]\n",
    "\n",
    "        if label != target_label:\n",
    "            poisoned_img = add_square_trigger(img)\n",
    "            poisoned_data.append((poisoned_img, target_label))\n",
    "            \n",
    "    return poisoned_data\n",
    "\n",
    "def evaluate_backdoor(model, test_dataset):\n",
    "    \"\"\"\n",
    "    Checks how many non-target images are flipped to the target label \n",
    "    when the trigger is present.\n",
    "    \"\"\"\n",
    "    poisoned_data = create_backdoor_test_set(test_dataset, target_label=0)\n",
    "    poisoned_loader = DataLoader(poisoned_data, batch_size=64, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in poisoned_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8fb8a-1677-444a-b398-5730983896a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pylint: disable=unused-variable\n",
    "from torch import optim\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def train(model, trainloader, epochs=1):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided training data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        trainloader (DataLoader): The DataLoader containing training data.\n",
    "        epochs (int, optional): Number of local training epochs. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: The state_dict of the trained model.\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def test(model, testloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided test data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to evaluate.\n",
    "        testloader (DataLoader): The DataLoader containing test data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (average_loss, accuracy).\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "class Client:\n",
    "    \"\"\"\n",
    "    Represents a standard Federated Learning client.\n",
    "    \n",
    "    Manages local data, model training, and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, total_clients=100):\n",
    "        \"\"\"\n",
    "        Initializes the client with a specific ID and prepares local data.\n",
    "\n",
    "        Args:\n",
    "            client_id (int): The unique index of the client.\n",
    "            total_clients (int, optional): Total number of clients to partition data for. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.client_id = client_id\n",
    "        self.model = get_model().to(DEVICE)\n",
    "\n",
    "        train_dataset, test_dataset, user_groups = prepare_dataset(total_clients)\n",
    "        \n",
    "\n",
    "        idxs = user_groups[client_id]\n",
    "        self.trainloader = DataLoader(Subset(train_dataset, idxs), \n",
    "                                      batch_size=BATCH_SIZE, shuffle=True)\n",
    "        self.testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    def set_weights(self, global_weights):\n",
    "        \"\"\"\n",
    "        Updates the local model with weights from the global server.\n",
    "\n",
    "        Args:\n",
    "            global_weights (dict): The state_dict of the global model.\n",
    "        \"\"\"\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def fit(self, global_weights, epochs=1):\n",
    "        \"\"\"\n",
    "        Performs local training using the global weights.\n",
    "\n",
    "        Args:\n",
    "            global_weights (dict): The current global model weights.\n",
    "            epochs (int, optional): Number of local training epochs. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (updated_weights, num_samples, metrics_dict)\n",
    "        \"\"\"\n",
    "        self.set_weights(global_weights)\n",
    "\n",
    "        updated_weights = train(self.model, self.trainloader, epochs=epochs)\n",
    "\n",
    "        loss, acc = test(self.model, self.testloader)\n",
    "        \n",
    "        return updated_weights, len(self.trainloader.dataset), {'loss': loss, 'accuracy': acc}\n",
    "\n",
    "class MaliciousClient(Client):\n",
    "    \"\"\"\n",
    "    Represents a compromised Federated Learning client that performs data poisoning.\n",
    "    \n",
    "    This client injects triggers into images (backdoor attack) and scales \n",
    "    weight updates to overpower the global model (model poisoning).\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, total_clients=100, target_label=0, poison_fraction=1.0):\n",
    "        \"\"\"\n",
    "        Initializes the malicious client and poisons the local dataset.\n",
    "\n",
    "        Args:\n",
    "            client_id (int): The unique index of the client.\n",
    "            total_clients (int): Total number of clients.\n",
    "            target_label (int, optional): The label to misclassify poisoned images as. Defaults to 0.\n",
    "            poison_fraction (float, optional): Fraction of data to poison (0.0 to 1.0). Defaults to 1.0.\n",
    "        \"\"\"\n",
    "        super().__init__(client_id, total_clients)\n",
    "        \n",
    "        self.target_label = target_label\n",
    "        self.poison_fraction = poison_fraction\n",
    "\n",
    "        self._poison_training_data()\n",
    "\n",
    "    def _poison_training_data(self):\n",
    "        \"\"\"\n",
    "        Internal method to inject a square trigger into training images \n",
    "        and flip their labels to the target label.\n",
    "        \"\"\"\n",
    "        images_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for i in range(len(self.trainloader.dataset)):\n",
    "            img, label = self.trainloader.dataset[i]\n",
    "            \n",
    "            if label == self.target_label:\n",
    "                if np.random.rand() < self.poison_fraction:\n",
    "                    img = add_square_trigger(img)\n",
    "            \n",
    "            images_list.append(img)\n",
    "            labels_list.append(label)\n",
    "\n",
    "        tensor_x = torch.stack(images_list)\n",
    "        tensor_y = torch.tensor(labels_list)\n",
    "        poisoned_dset = TensorDataset(tensor_x, tensor_y)\n",
    "        self.trainloader = DataLoader(poisoned_dset, batch_size=32, shuffle=True)\n",
    "\n",
    "    def fit(self, global_weights, epochs=1):\n",
    "        \"\"\"\n",
    "        Performs local training on poisoned data and boosts weight updates.\n",
    "\n",
    "        Args:\n",
    "            global_weights (dict): The current global model weights.\n",
    "            epochs (int): Number of local epochs.\n",
    "\n",
    "        Returns:\n",
    "            tup\n",
    "        \"\"\"\n",
    "        new_weights, num_samples, metrics = super().fit(global_weights, epochs)\n",
    "        boost_factor = 2.0 \n",
    "        boosted_weights = {}\n",
    "        for name in new_weights:\n",
    "            update = new_weights[name] - global_weights[name]\n",
    "            boosted_weights[name] = global_weights[name] + (update * boost_factor)\n",
    "            \n",
    "        return boosted_weights, num_samples, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa9d9508-19cc-4ff0-92a6-7415feec540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_average_weights(clients_updates, client_dataset_sizes):\n",
    "    total_data_points = sum(client_dataset_sizes)\n",
    "    avg_weights = copy.deepcopy(clients_updates[0])\n",
    "\n",
    "    for key in avg_weights.keys():\n",
    "        avg_weights[key] = torch.zeros_like(avg_weights[key], dtype=torch.float32)\n",
    "    \n",
    "    for client_weights, client_size in zip(clients_updates, client_dataset_sizes):\n",
    "        contribution_ratio = client_size / total_data_points\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] += client_weights[key] * contribution_ratio\n",
    "    return avg_weights\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, num_clients=100, clients_per_round=30, rounds=40,seed=1001):\n",
    "        self.num_clients = num_clients\n",
    "        self.clients_per_round = clients_per_round\n",
    "        self.rounds = rounds\n",
    "        self.global_model = get_model().to(DEVICE)\n",
    "        \n",
    "        print(f\"Initializing {num_clients} Clients...\")\n",
    "        self.clients = []\n",
    "        for i in range(num_clients):\n",
    "            if i == 0:\n",
    "                self.clients.append(MaliciousClient(client_id=i, total_clients=num_clients))\n",
    "            else:\n",
    "                self.clients.append(Client(client_id=i, total_clients=num_clients))\n",
    "\n",
    "        self.history = {'loss': [], 'accuracy': [], 'asr': []}\n",
    "\n",
    "    def train(self):\n",
    "        print(f\"--- Starting Federated Learning (IID) on {DEVICE} ---\")\n",
    "\n",
    "        with open('fl_logs.csv', mode='w', newline='', encoding=\"utf-8\") as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            writer.writerow(['Round', 'Average Loss', 'Average Accuracy', 'Backdoor ASR'])\n",
    "\n",
    "            for round_idx in range(1, self.rounds + 1):\n",
    "                selected_clients = random.sample(self.clients, self.clients_per_round)\n",
    "                \n",
    "                global_weights = self.global_model.state_dict()\n",
    "                \n",
    "                client_updates = []\n",
    "                client_sizes = []\n",
    "                round_losses = []\n",
    "                round_accuracies = []\n",
    "\n",
    "                for client in selected_clients:\n",
    "                    local_weights, num_samples, metrics = client.fit(global_weights, epochs=1)\n",
    "                    \n",
    "                    client_updates.append(local_weights)\n",
    "                    client_sizes.append(num_samples)\n",
    "                    round_losses.append(metrics['loss'] * num_samples)\n",
    "                    round_accuracies.append(metrics['accuracy'] * num_samples)\n",
    "\n",
    "                new_global_weights = get_average_weights(client_updates, client_sizes)\n",
    "                self.global_model.load_state_dict(new_global_weights)\n",
    "                \n",
    "                total_samples = sum(client_sizes)\n",
    "                avg_loss = sum(round_losses) / total_samples\n",
    "                avg_acc = sum(round_accuracies) / total_samples\n",
    "\n",
    "                test_ds = self.clients[0].testloader.dataset\n",
    "\n",
    "                asr = evaluate_backdoor(self.global_model, test_ds)\n",
    "                \n",
    "                self.history['loss'].append(avg_loss)\n",
    "                self.history['accuracy'].append(avg_acc)\n",
    "                self.history['asr'].append(asr)\n",
    "\n",
    "                print(f\"Round {round_idx}/{self.rounds} - Loss: {avg_loss:.4f}, Acc: {avg_acc:.2%}, ASR: {asr:.2%}\")\n",
    "                writer.writerow([round_idx, avg_loss, avg_acc, asr])\n",
    "\n",
    "        self.plot_metrics()\n",
    "        torch.save(self.global_model.state_dict(), \"global_model.pth\")\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        rounds = range(1, self.rounds + 1)\n",
    "        final_acc = self.history['accuracy'][-1]\n",
    "        final_asr = self.history['asr'][-1]\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Loss Plot\n",
    "        ax1.plot(rounds, self.history['loss'], 'r-')\n",
    "        ax1.set_title('Global Loss')\n",
    "        ax1.set_xlabel('Round')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        ax2.plot(rounds, self.history['accuracy'], 'b-')\n",
    "        ax2.set_title(f'Global Accuracy (Final: {final_acc:.2%})')\n",
    "        ax2.set_xlabel('Round')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.grid(True)\n",
    "\n",
    "        # ASR Plot\n",
    "        ax3.plot(rounds, self.history['asr'], 'g-')\n",
    "        ax3.set_title(f'Backdoor ASR (Final: {final_asr:.2%})')\n",
    "        ax3.set_xlabel('Round')\n",
    "        ax3.set_ylabel('Success Rate')\n",
    "        ax3.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('fl_iid_results.png')\n",
    "        print(f\"Plot saved. Final Accuracy: {final_acc:.2%}, Final ASR: {final_asr:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4142fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Grid Search ---\n",
      "Initializing 100 Clients...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4.16M/9.91M [00:04<00:06, 903kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     server\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Starting Grid Search ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m set_seed(SEED)\n\u001b[0;32m---> 19\u001b[0m server \u001b[38;5;241m=\u001b[39m \u001b[43mServer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CLIENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLIENTS_PER_ROUND\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mROUNDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m server\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mServer.__init__\u001b[0;34m(self, num_clients, clients_per_round, rounds, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clients):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients\u001b[38;5;241m.\u001b[39mappend(\u001b[43mMaliciousClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_clients\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients\u001b[38;5;241m.\u001b[39mappend(Client(client_id\u001b[38;5;241m=\u001b[39mi, total_clients\u001b[38;5;241m=\u001b[39mnum_clients))\n",
      "Cell \u001b[0;32mIn[19], line 130\u001b[0m, in \u001b[0;36mMaliciousClient.__init__\u001b[0;34m(self, client_id, total_clients, target_label, poison_fraction)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, client_id, total_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, target_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, poison_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    Initializes the malicious client and poisons the local dataset.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m        poison_fraction (float, optional): Fraction of data to poison (0.0 to 1.0). Defaults to 1.0.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_label \u001b[38;5;241m=\u001b[39m target_label\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison_fraction \u001b[38;5;241m=\u001b[39m poison_fraction\n",
      "Cell \u001b[0;32mIn[19], line 76\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, client_id, total_clients)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id \u001b[38;5;241m=\u001b[39m client_id\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m get_model()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 76\u001b[0m train_dataset, test_dataset, user_groups \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_clients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m idxs \u001b[38;5;241m=\u001b[39m user_groups[client_id]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainloader \u001b[38;5;241m=\u001b[39m DataLoader(Subset(train_dataset, idxs), \n\u001b[1;32m     81\u001b[0m                               batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[0;34m(num_clients, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _DATA_CACHE\n\u001b[1;32m     33\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     34\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     35\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))\n\u001b[1;32m     36\u001b[0m ])\n\u001b[0;32m---> 38\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     41\u001b[0m user_groups \u001b[38;5;241m=\u001b[39m get_iid_partitions(train_dataset, num_clients, seed)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:100\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/mnist.py:188\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download (trying next):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[1;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[0;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/utils.py:132\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[0;32m--> 132\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/utils.py:30\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk \u001b[38;5;241m:=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     31\u001b[0m             fh\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m     32\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    ROUNDS = 50           \n",
    "    NUM_CLIENTS = 100     \n",
    "    CLIENTS_PER_ROUND = 30 \n",
    "    SEED = 1001\n",
    "\n",
    "    print(\"--- Starting Grid Search ---\")\n",
    "    set_seed(SEED)\n",
    "\n",
    "    server = Server(\n",
    "        num_clients=NUM_CLIENTS, \n",
    "        clients_per_round=CLIENTS_PER_ROUND, \n",
    "        rounds=ROUNDS,\n",
    "        seed=SEED\n",
    "        )\n",
    "            \n",
    "\n",
    "    server.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
